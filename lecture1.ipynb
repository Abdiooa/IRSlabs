{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in ./venv/env/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in ./venv/env/lib/python3.8/site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (2024.2.2)\n",
      "Requirement already satisfied: razdel in ./venv/env/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim in ./venv/env/lib/python3.8/site-packages (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./venv/env/lib/python3.8/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./venv/env/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: six>=1.5.0 in ./venv/env/lib/python3.8/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in ./venv/env/lib/python3.8/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: nltk in ./venv/env/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./venv/env/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./venv/env/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/env/lib/python3.8/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in ./venv/env/lib/python3.8/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: rusenttokenize in ./venv/env/lib/python3.8/site-packages (0.0.5)\n",
      "Requirement already satisfied: regex in ./venv/env/lib/python3.8/site-packages (2023.12.25)\n",
      "Requirement already satisfied: pip in ./venv/env/lib/python3.8/site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3\n",
    "!pip install razdel\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install rusenttokenize\n",
    "!pip install regex\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"ДАННОЕ СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО \"\\\n",
    "       \"ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ \"\\\n",
    "       \"ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, \"\\\n",
    "       \"ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ф', 'Ф', 'Ф']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('Ф', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ДАННаЕ СааБЩЕНИЕ (МАТЕРИАЛ) СаЗДАНа И (ИЛИ) РАСПРаСТРАНЕНа ИНаСТРАННЫМ СРЕДСТВаМ МАССаВаЙ ИНФаРМАЦИИ, ВЫПаЛНЯЮЩИМ ФУНКЦИИ ИНаСТРАННаГа АГЕНТА, И (ИЛИ) РаССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦаМ, ВЫПаЛНЯЮЩИМ ФУНКЦИИ ИНаСТРАННаГа АГЕНТА'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('О', 'а', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ф', 'Ф', 'Ф']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in text if t == 'Ф']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ДАННaЕ СaaБЩЕНИЕ (МАТЕРИАЛ) СaЗДАНa И (ИЛИ) РАСПРaСТРАНЕНa ИНaСТРАННЫМ СРЕДСТВaМ МАССaВaЙ ИНФaРМАЦИИ, ВЫПaЛНЯЮЩИМ ФУНКЦИИ ИНaСТРАННaГa АГЕНТА, И (ИЛИ) РaССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦaМ, ВЫПaЛНЯЮЩИМ ФУНКЦИИ ИНaСТРАННaГa АГЕНТА'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(\"О\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ф', 'Ц', 'Ф', 'Ц', 'Ц', 'Ф', 'Ц']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[ФЦ]\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Д',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'Е',\n",
       " 'С',\n",
       " 'О',\n",
       " 'О',\n",
       " 'Б',\n",
       " 'Щ',\n",
       " 'Е',\n",
       " 'Н',\n",
       " 'И',\n",
       " 'Е',\n",
       " 'М',\n",
       " 'А',\n",
       " 'Т',\n",
       " 'Е',\n",
       " 'Р',\n",
       " 'И',\n",
       " 'А',\n",
       " 'Л',\n",
       " 'С',\n",
       " 'О',\n",
       " 'З',\n",
       " 'Д',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'И',\n",
       " 'И',\n",
       " 'Л',\n",
       " 'И',\n",
       " 'Р',\n",
       " 'А',\n",
       " 'С',\n",
       " 'П',\n",
       " 'Р',\n",
       " 'О',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'Р',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'Е',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'И',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'Р',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'Н',\n",
       " 'Ы',\n",
       " 'М',\n",
       " 'С',\n",
       " 'Р',\n",
       " 'Е',\n",
       " 'Д',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'В',\n",
       " 'О',\n",
       " 'М',\n",
       " 'М',\n",
       " 'А',\n",
       " 'С',\n",
       " 'С',\n",
       " 'О',\n",
       " 'В',\n",
       " 'О',\n",
       " 'Й',\n",
       " 'И',\n",
       " 'Н',\n",
       " 'Ф',\n",
       " 'О',\n",
       " 'Р',\n",
       " 'М',\n",
       " 'А',\n",
       " 'Ц',\n",
       " 'И',\n",
       " 'И',\n",
       " 'В',\n",
       " 'Ы',\n",
       " 'П',\n",
       " 'О',\n",
       " 'Л',\n",
       " 'Н',\n",
       " 'Я',\n",
       " 'Ю',\n",
       " 'Щ',\n",
       " 'И',\n",
       " 'М',\n",
       " 'Ф',\n",
       " 'У',\n",
       " 'Н',\n",
       " 'К',\n",
       " 'Ц',\n",
       " 'И',\n",
       " 'И',\n",
       " 'И',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'Р',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'Г',\n",
       " 'О',\n",
       " 'А',\n",
       " 'Г',\n",
       " 'Е',\n",
       " 'Н',\n",
       " 'Т',\n",
       " 'А',\n",
       " 'И',\n",
       " 'И',\n",
       " 'Л',\n",
       " 'И',\n",
       " 'Р',\n",
       " 'О',\n",
       " 'С',\n",
       " 'С',\n",
       " 'И',\n",
       " 'Й',\n",
       " 'С',\n",
       " 'К',\n",
       " 'И',\n",
       " 'М',\n",
       " 'Ю',\n",
       " 'Р',\n",
       " 'И',\n",
       " 'Д',\n",
       " 'И',\n",
       " 'Ч',\n",
       " 'Е',\n",
       " 'С',\n",
       " 'К',\n",
       " 'И',\n",
       " 'М',\n",
       " 'Л',\n",
       " 'И',\n",
       " 'Ц',\n",
       " 'О',\n",
       " 'М',\n",
       " 'В',\n",
       " 'Ы',\n",
       " 'П',\n",
       " 'О',\n",
       " 'Л',\n",
       " 'Н',\n",
       " 'Я',\n",
       " 'Ю',\n",
       " 'Щ',\n",
       " 'И',\n",
       " 'М',\n",
       " 'Ф',\n",
       " 'У',\n",
       " 'Н',\n",
       " 'К',\n",
       " 'Ц',\n",
       " 'И',\n",
       " 'И',\n",
       " 'И',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'С',\n",
       " 'Т',\n",
       " 'Р',\n",
       " 'А',\n",
       " 'Н',\n",
       " 'Н',\n",
       " 'О',\n",
       " 'Г',\n",
       " 'О',\n",
       " 'А',\n",
       " 'Г',\n",
       " 'Е',\n",
       " 'Н',\n",
       " 'Т',\n",
       " 'А']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[А-Я]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[а-я]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[А-Яа-я]\", '', text) == re.sub(\"[А-я]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['А',\n",
       " ' ',\n",
       " 'Б',\n",
       " ' ',\n",
       " '(',\n",
       " 'А',\n",
       " 'А',\n",
       " ')',\n",
       " ' ',\n",
       " 'А',\n",
       " ' ',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " ' ',\n",
       " 'А',\n",
       " 'А',\n",
       " ' ',\n",
       " 'А',\n",
       " ' ',\n",
       " 'В',\n",
       " ' ',\n",
       " 'А',\n",
       " 'В',\n",
       " ' ',\n",
       " 'А',\n",
       " ',',\n",
       " ' ',\n",
       " 'В',\n",
       " ' ',\n",
       " ' ',\n",
       " 'А',\n",
       " 'Г',\n",
       " ' ',\n",
       " 'А',\n",
       " 'Г',\n",
       " 'А',\n",
       " ',',\n",
       " ' ',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ',',\n",
       " ' ',\n",
       " 'В',\n",
       " ' ',\n",
       " ' ',\n",
       " 'А',\n",
       " 'Г',\n",
       " ' ',\n",
       " 'А',\n",
       " 'Г',\n",
       " 'А']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[^Д-Я]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'____________________________________________________________________________________________________________________________________________________________________________________________________________________________'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\D', '_', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ДАННОЕ',\n",
       " 'СООБЩЕНИЕ',\n",
       " '(МАТЕРИАЛ)',\n",
       " 'СОЗДАНО',\n",
       " 'И',\n",
       " '(ИЛИ)',\n",
       " 'РАСПРОСТРАНЕНО',\n",
       " 'ИНОСТРАННЫМ',\n",
       " 'СРЕДСТВОМ',\n",
       " 'МАССОВОЙ',\n",
       " 'ИНФОРМАЦИИ,',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА,',\n",
       " 'И',\n",
       " '(ИЛИ)',\n",
       " 'РОССИЙСКИМ',\n",
       " 'ЮРИДИЧЕСКИМ',\n",
       " 'ЛИЦОМ,',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(\"\\s\",  text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\".\", '', text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ДАННОЕ',\n",
       " 'СООБЩЕНИЕ',\n",
       " 'МАТЕРИАЛ',\n",
       " 'СОЗДАНО',\n",
       " 'И',\n",
       " 'ИЛИ',\n",
       " 'РАСПРОСТРАНЕНО',\n",
       " 'ИНОСТРАННЫМ',\n",
       " 'СРЕДСТВОМ',\n",
       " 'МАССОВОЙ',\n",
       " 'ИНФОРМАЦИИ',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА',\n",
       " 'И',\n",
       " 'ИЛИ',\n",
       " 'РОССИЙСКИМ',\n",
       " 'ЮРИДИЧЕСКИМ',\n",
       " 'ЛИЦОМ',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ИНФОРМАЦИИ,', 'АГЕНТА,', 'ЛИЦОМ,']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+,', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ДАННОЕ',\n",
       " 'СООБЩЕНИЕ',\n",
       " 'МАТЕРИАЛ',\n",
       " 'СОЗДАНО',\n",
       " 'И',\n",
       " 'ИЛИ',\n",
       " 'РАСПРОСТРАНЕНО',\n",
       " 'ИНОСТРАННЫМ',\n",
       " 'СРЕДСТВОМ',\n",
       " 'МАССОВОЙ',\n",
       " 'ИНФОРМАЦИИ,',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА,',\n",
       " 'И',\n",
       " 'ИЛИ',\n",
       " 'РОССИЙСКИМ',\n",
       " 'ЮРИДИЧЕСКИМ',\n",
       " 'ЛИЦОМ,',\n",
       " 'ВЫПОЛНЯЮЩИМ',\n",
       " 'ФУНКЦИИ',\n",
       " 'ИНОСТРАННОГО',\n",
       " 'АГЕНТА']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+,?', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО ']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\s.+\\s', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\\n\\nВо-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\\n\\nВо-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
    "\n",
    "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
    "\n",
    "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\n",
    "\n",
    "\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sent_tokenize(text, 'russian')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(split_sentences(text))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "ru_sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           244,\n",
       "           'Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.'),\n",
       " Substring(245,\n",
       "           322,\n",
       "           'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.'),\n",
       " Substring(324,\n",
       "           401,\n",
       "           'Во-первых, NLI можно использовать для контроля качества генеративных моделей.'),\n",
       " Substring(402,\n",
       "           635,\n",
       "           'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.'),\n",
       " Substring(636,\n",
       "           913,\n",
       "           'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(sentenize(text))\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.text for sent in sents[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE',\n",
       " 'роме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\\n\\nВо-первых, NLI можно использовать для контроля качества генеративных моделей',\n",
       " 'сть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод',\n",
       " 'овременные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\"',\n",
       " ' помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\\n\\nВо-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов',\n",
       " 'ля русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше',\n",
       " ' статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично',\n",
       " 'оэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\\n\\n']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[!?\\.] [А-Я]', text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть,',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо,',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " '\"плывёт\";',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~«»—…“”'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation += '«»—…“”'\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " 'плывёт',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.strip(string.punctuation) for word in text.split()][10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как-нибудь'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'как-нибудь'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'т.е'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'т.е.'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лажают',\n",
       " ',',\n",
       " 'упуская',\n",
       " 'какую-то',\n",
       " 'важную',\n",
       " 'информацию',\n",
       " 'из',\n",
       " 'Х',\n",
       " ',',\n",
       " 'или',\n",
       " ',',\n",
       " 'наоборот',\n",
       " ',',\n",
       " 'дописывая',\n",
       " 'в',\n",
       " 'текст',\n",
       " 'Y',\n",
       " 'что-то',\n",
       " 'нафантазированное',\n",
       " '``']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text=text)[130:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['russiansuperglue',\n",
       " 'кроме',\n",
       " 'этого',\n",
       " 'модели',\n",
       " 'nli',\n",
       " 'обладают',\n",
       " 'прикладной',\n",
       " 'ценностью',\n",
       " 'по',\n",
       " 'нескольким',\n",
       " 'причинам',\n",
       " 'во',\n",
       " 'первых',\n",
       " 'nli',\n",
       " 'можно',\n",
       " 'использовать',\n",
       " 'для',\n",
       " 'контроля',\n",
       " 'качества',\n",
       " 'генеративных']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "list(tokenize(text, lowercase=True))[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Задача'),\n",
       " Substring(7, 10, 'NLI'),\n",
       " Substring(11, 16, 'важна'),\n",
       " Substring(17, 20, 'для'),\n",
       " Substring(21, 33, 'компьютерных'),\n",
       " Substring(34, 44, 'лингвистов'),\n",
       " Substring(44, 45, ','),\n",
       " Substring(46, 49, 'ибо'),\n",
       " Substring(50, 53, 'она'),\n",
       " Substring(54, 63, 'позволяет')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize as razdel_tokenize\n",
    "list(razdel_tokenize(text))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text.lower() for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Задача', 'задач'),\n",
       " ('NLI', 'NLI'),\n",
       " ('важна', 'важн'),\n",
       " ('для', 'для'),\n",
       " ('компьютерных', 'компьютерн'),\n",
       " ('лингвистов', 'лингвист'),\n",
       " (',', ','),\n",
       " ('ибо', 'иб'),\n",
       " ('она', 'он'),\n",
       " ('позволяет', 'позволя'),\n",
       " ('детально', 'детальн'),\n",
       " ('рассмотреть', 'рассмотрет'),\n",
       " (',', ','),\n",
       " ('какие', 'как'),\n",
       " ('языковые', 'языков'),\n",
       " ('явления', 'явлен'),\n",
       " ('данная', 'дан'),\n",
       " ('модель', 'модел'),\n",
       " ('понимает', 'понима'),\n",
       " ('хорошо', 'хорош'),\n",
       " (',', ','),\n",
       " ('а', 'а'),\n",
       " ('на', 'на'),\n",
       " ('каких', 'как'),\n",
       " ('–', '–'),\n",
       " ('``', '``'),\n",
       " ('плывёт', 'плывет'),\n",
       " (\"''\", \"''\"),\n",
       " (';', ';'),\n",
       " ('по', 'по')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('russian')\n",
    "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in ./venv/env/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in ./venv/env/lib/python3.8/site-packages (from pymystem3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests->pymystem3) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import os, json\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " ' ',\n",
       " 'NLI',\n",
       " ' ',\n",
       " 'важный',\n",
       " ' ',\n",
       " 'для',\n",
       " ' ',\n",
       " 'компьютерный',\n",
       " ' ',\n",
       " 'лингвист',\n",
       " ', ',\n",
       " 'ибо',\n",
       " ' ',\n",
       " 'она',\n",
       " ' ',\n",
       " 'позволять',\n",
       " ' ',\n",
       " 'детально',\n",
       " ' ',\n",
       " 'рассматривать',\n",
       " ', ',\n",
       " 'какой',\n",
       " ' ',\n",
       " 'языковой',\n",
       " ' ',\n",
       " 'явление',\n",
       " ' ',\n",
       " 'данный',\n",
       " ' ']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem.lemmatize(text)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_analized = mystem.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'wt': 1, 'gr': 'A=ед,кр,жен'}],\n",
       "  'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный',\n",
       "    'wt': 1,\n",
       "    'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_analized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово -  Задача\n",
      "Разбор слова -  {'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}\n",
      "Лемма слова -  задача\n",
      "Грамматическая информация слова -  S,жен,неод=им,ед\n"
     ]
    }
   ],
   "source": [
    "print('Слово - ', words_analized[0]['text'])\n",
    "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
    "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
    "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'важный',\n",
       " 'для',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассматривать']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[parse['analysis'][0]['lex'] for parse in words_analized if parse.get('analysis')][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: mystem: command not found\n"
     ]
    }
   ],
   "source": [
    "!mystem -isc --format json text.txt text_parsed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = [json.loads(line) for line in open('text_parsed.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'gr': 'S,жен,неод=им,ед'}], 'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'gr': 'A=ед,кр,жен'}], 'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный', 'gr': 'A=пр,мн,полн'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=вин,мн,полн,од'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=род,мн,полн'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/kmike/pymorphy2.git\n",
      "  Cloning https://github.com/kmike/pymorphy2.git to /tmp/pip-req-build-_k4tkiv6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/kmike/pymorphy2.git /tmp/pip-req-build-_k4tkiv6\n",
      "  Resolved https://github.com/kmike/pymorphy2.git to commit 92d546f042ff14601376d3646242908d5ab786c1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dawg-python>=0.7.1 in ./venv/env/lib/python3.8/site-packages (from pymorphy2==0.9.1) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in ./venv/env/lib/python3.8/site-packages (from pymorphy2==0.9.1) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in ./venv/env/lib/python3.8/site-packages (from pymorphy2==0.9.1) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/kmike/pymorphy2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_analized = [morph.parse(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='печь', score=0.571428, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('INFN,impf,tran'), normal_form='печь', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'печь', 2456, 0),)),\n",
       " Parse(word='печь', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='печь', score=0.142857, methods_stack=((DictionaryAnalyzer(), 'печь', 2223, 3),))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"печь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово -  задача\n",
      "Разбор первого слова -  Parse(word='задача', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='задача', score=1.0, methods_stack=((DictionaryAnalyzer(), 'задача', 94, 0),))\n",
      "Лемма первого слова -  задача\n",
      "Грамматическая информация первого слова -  NOUN,inan,femn sing,nomn\n",
      "Часть речи первого слова -  NOUN\n",
      "Род первого слова -  femn\n",
      "Число первого слова -  sing\n",
      "Падеж первого слова -  nomn\n"
     ]
    }
   ],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Первое слово - ', words_analized[0][0].word)\n",
    "print('Разбор первого слова - ', words_analized[0][0])\n",
    "print('Лемма первого слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация первого слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи первого слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род первого слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число первого слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж первого слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aoo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важный',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассмотреть']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in word_tokenize(text)]\n",
    "[word for word in words_normalized if word not in stops][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/env/lib/python3.8/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/env/lib/python3.8/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/env/lib/python3.8/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m112.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from spacy)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/env/lib/python3.8/site-packages (from spacy) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/env/lib/python3.8/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m173.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in ./venv/env/lib/python3.8/site-packages (from spacy) (1.24.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading spacy-3.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m45.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0mm\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m133.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m84.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m54.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m50.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.3/494.3 kB\u001b[0m \u001b[31m59.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (934 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.4/934.4 kB\u001b[0m \u001b[31m53.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m155.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m180.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m153.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m31.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:08\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m150.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, MarkupSafe, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, jinja2, confection, weasel, thinc, spacy\n",
      "Successfully installed MarkupSafe-2.1.5 annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 jinja2-3.1.3 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.1 pydantic-core-2.16.2 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m173.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in ./venv/env/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/env/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m197.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in ./venv/env/lib/python3.8/site-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/env/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m235.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in ./venv/env/lib/python3.8/site-packages (from ru-core-news-sm==3.7.0) (3.7.4)\n",
      "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3-2.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in ./venv/env/lib/python3.8/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0) (0.7.2)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m179.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./venv/env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/env/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\n",
      "Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m182.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-sm\n",
      "Successfully installed pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для английского языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"One of the most salient features of our culture is that it won't so much bullshit.” \"\n",
    "        \"These are the opening words of the short book On Bullshit, written by the philosopher Harry Frankfurt. \"\n",
    "        \"Fifteen years after the publication of this surprise bestseller, \"\n",
    "        \"the rapid progress of research on artificial intelligence is forcing us to reconsider our conception \"\n",
    "        \"of bullshit as a hallmark of human speech, with troubling implications. What do philosophical \"\n",
    "        \"reflections on bullshit have to do with algorithms? As it turns out, quite a lot.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One one NUM\n",
      "of of ADP\n",
      "the the DET\n",
      "most most ADV\n",
      "salient salient ADJ\n",
      "features feature NOUN\n",
      "of of ADP\n",
      "our our PRON\n",
      "culture culture NOUN\n",
      "is be AUX\n",
      "that that SCONJ\n",
      "it it PRON\n",
      "wo will AUX\n",
      "n't not PART\n",
      "so so ADV\n",
      "much much ADJ\n",
      "bullshit bullshit NOUN\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      "These these PRON\n",
      "are be AUX\n",
      "the the DET\n",
      "opening opening NOUN\n",
      "words word NOUN\n",
      "of of ADP\n",
      "the the DET\n",
      "short short ADJ\n",
      "book book NOUN\n",
      "On on ADP\n",
      "Bullshit Bullshit PROPN\n",
      ", , PUNCT\n",
      "written write VERB\n",
      "by by ADP\n",
      "the the DET\n",
      "philosopher philosopher NOUN\n",
      "Harry Harry PROPN\n",
      "Frankfurt Frankfurt PROPN\n",
      ". . PUNCT\n",
      "\n",
      "Fifteen fifteen NUM\n",
      "years year NOUN\n",
      "after after ADP\n",
      "the the DET\n",
      "publication publication NOUN\n",
      "of of ADP\n",
      "this this DET\n",
      "surprise surprise NOUN\n",
      "bestseller bestseller NOUN\n",
      ", , PUNCT\n",
      "the the DET\n",
      "rapid rapid ADJ\n",
      "progress progress NOUN\n",
      "of of ADP\n",
      "research research NOUN\n",
      "on on ADP\n",
      "artificial artificial ADJ\n",
      "intelligence intelligence NOUN\n",
      "is be AUX\n",
      "forcing force VERB\n",
      "us we PRON\n",
      "to to PART\n",
      "reconsider reconsider VERB\n",
      "our our PRON\n",
      "conception conception NOUN\n",
      "of of ADP\n",
      "bullshit bullshit NOUN\n",
      "as as ADP\n",
      "a a DET\n",
      "hallmark hallmark NOUN\n",
      "of of ADP\n",
      "human human ADJ\n",
      "speech speech NOUN\n",
      ", , PUNCT\n",
      "with with ADP\n",
      "troubling troubling ADJ\n",
      "implications implication NOUN\n",
      ". . PUNCT\n",
      "\n",
      "What what PRON\n",
      "do do AUX\n",
      "philosophical philosophical ADJ\n",
      "reflections reflection NOUN\n",
      "on on ADP\n",
      "bullshit bullshit NOUN\n",
      "have have VERB\n",
      "to to PART\n",
      "do do VERB\n",
      "with with ADP\n",
      "algorithms algorithm NOUN\n",
      "? ? PUNCT\n",
      "\n",
      "As as SCONJ\n",
      "it it PRON\n",
      "turns turn VERB\n",
      "out out ADP\n",
      ", , PUNCT\n",
      "quite quite DET\n",
      "a a DET\n",
      "lot lot NOUN\n",
      ". . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the most salient features', 'our culture', 'it', 'These', 'the opening words', 'the short book', 'Bullshit', 'the philosopher', 'Harry Frankfurt', 'the publication', 'this surprise bestseller', 'the rapid progress', 'research', 'artificial intelligence', 'us', 'our conception', 'bullshit', 'a hallmark', 'human speech', 'troubling implications', 'What', 'philosophical reflections', 'bullshit', 'algorithms', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для немецкого языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Vor den Stadien habe ich bis jetzt zum Glück noch keine wüsten Szenen gesehen.\"\n",
    "        \"Vorstandschef Timotheus Höttges habe sich ausgesprochen optimistisch gezeigt, schrieb Analyst Robert Grindle in einer Studie vom Montag. \"\n",
    "        \"Während der dortigen Räterepublik war er nach dem Krieg in Künstlergruppen und Ausschüssen aktiv.\"\n",
    "        \"Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für die\"\n",
    "        \"Menschen verträglicher gestaltet wird“, so Gusenbauer.\"\n",
    "        \"Weitere Informationen unter www.schnippenburg.de sowie www.eisenzeithaus.de. Es gibt neue Nachrichten auf noz.de!\" \n",
    "        \"Jetzt die Startseite neu laden.\"\n",
    "        \"Der Initiative 'Zivilcourage', die sich jahrelang für das Denkmal in Form eines offenen \" \n",
    "        \"Der islamistischen Szene Thüringens wurden nach Angaben des Thüringer Innenministeriums \"\n",
    "        \"zuletzt etwa 125 Personen zugerechnet, der salafistischen Szene etwa 75 Personen.\"\n",
    "        \"Allerdings bestand er die EMV-Prüfung nicht, weil er Radios und DVB-T-Empfänger stört.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor vor ADP\n",
      "den der DET\n",
      "Stadien Stadien NOUN\n",
      "habe haben AUX\n",
      "ich ich PRON\n",
      "bis bis ADP\n",
      "jetzt jetzt ADV\n",
      "zum zu ADP\n",
      "Glück Glück NOUN\n",
      "noch noch ADV\n",
      "keine kein DET\n",
      "wüsten wüst ADJ\n",
      "Szenen Szene NOUN\n",
      "gesehen sehen VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Vorstandschef Vorstandschef NOUN\n",
      "Timotheus Timotheus PROPN\n",
      "Höttges Höttges PROPN\n",
      "habe haben AUX\n",
      "sich sich PRON\n",
      "ausgesprochen ausgesprochen ADV\n",
      "optimistisch optimistisch ADV\n",
      "gezeigt zeigen VERB\n",
      ", -- PUNCT\n",
      "schrieb schreiben VERB\n",
      "Analyst Analyst NOUN\n",
      "Robert Robert PROPN\n",
      "Grindle Grindle PROPN\n",
      "in in ADP\n",
      "einer ein DET\n",
      "Studie Studie NOUN\n",
      "vom von ADP\n",
      "Montag Montag NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Während während ADP\n",
      "der der DET\n",
      "dortigen dortig ADJ\n",
      "Räterepublik Räterepublik NOUN\n",
      "war sein AUX\n",
      "er er PRON\n",
      "nach nach ADP\n",
      "dem der DET\n",
      "Krieg Krieg NOUN\n",
      "in in ADP\n",
      "Künstlergruppen Künstlergruppe NOUN\n",
      "und und CCONJ\n",
      "Ausschüssen Ausschuß NOUN\n",
      "aktiv aktiv ADV\n",
      ". -- PUNCT\n",
      "\n",
      "Welches welcher DET\n",
      "Ergebnis Ergebnis NOUN\n",
      "die der DET\n",
      "Diskussion Diskussion NOUN\n",
      "auf auf ADP\n",
      "EU-Ebene EU-Eben NOUN\n",
      "auch auch ADV\n",
      "letztlich letztlich ADV\n",
      "bringt bringen VERB\n",
      ", -- PUNCT\n",
      "wichtig wichtig ADV\n",
      "ist sein AUX\n",
      ", -- PUNCT\n",
      "dass dass SCONJ\n",
      "die der DET\n",
      "Preisentwicklung Preisentwicklung NOUN\n",
      "für für ADP\n",
      "dieMenschen dieMenschen NOUN\n",
      "verträglicher verträglich ADV\n",
      "gestaltet gestalten VERB\n",
      "wird werden AUX\n",
      "“ -- PUNCT\n",
      ", -- PUNCT\n",
      "so so ADV\n",
      "Gusenbauer Gusenbauer PROPN\n",
      ". -- PUNCT\n",
      "\n",
      "Weitere weit ADJ\n",
      "Informationen Information NOUN\n",
      "unter unter ADP\n",
      "www.schnippenburg.de www.schnippenburg.de PROPN\n",
      "sowie sowie CCONJ\n",
      "www.eisenzeithaus.de www.eisenzeithaus.de NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Es es PRON\n",
      "gibt geben VERB\n",
      "neue neu ADJ\n",
      "Nachrichten Nachricht NOUN\n",
      "auf auf ADP\n",
      "noz.de noz.de PROPN\n",
      "! -- PUNCT\n",
      "\n",
      "Jetzt jetzt ADV\n",
      "die der DET\n",
      "Startseite Startseit NOUN\n",
      "neu neu ADV\n",
      "laden laden VERB\n",
      ". -- PUNCT\n",
      "\n",
      "Der der DET\n",
      "Initiative Initiative NOUN\n",
      "' ' PUNCT\n",
      "Zivilcourage Zivilcourage NOUN\n",
      "' -- PUNCT\n",
      ", -- PUNCT\n",
      "die der PRON\n",
      "sich sich PRON\n",
      "jahrelang jahrelang ADV\n",
      "für für ADP\n",
      "das der DET\n",
      "Denkmal Denkmal NOUN\n",
      "in in ADP\n",
      "Form Form NOUN\n",
      "eines ein DET\n",
      "offenen offen ADJ\n",
      "Der der DET\n",
      "islamistischen islamistisch ADJ\n",
      "Szene Szene NOUN\n",
      "Thüringens Thüringen PROPN\n",
      "wurden werden AUX\n",
      "nach nach ADP\n",
      "Angaben Angabe NOUN\n",
      "des der DET\n",
      "Thüringer Thüringer ADJ\n",
      "Innenministeriums Innenministerium NOUN\n",
      "zuletzt zuletzt ADV\n",
      "etwa etwa ADV\n",
      "125 125 NUM\n",
      "Personen Person NOUN\n",
      "zugerechnet zurechnen VERB\n",
      ", -- PUNCT\n",
      "der der DET\n",
      "salafistischen salafistisch ADJ\n",
      "Szene Szene NOUN\n",
      "etwa etwa ADV\n",
      "75 75 NUM\n",
      "Personen Person NOUN\n",
      ". -- PUNCT\n",
      "\n",
      "Allerdings allerdings ADV\n",
      "bestand bestehen VERB\n",
      "er er PRON\n",
      "die der DET\n",
      "EMV-Prüfung EMV-Prüfung NOUN\n",
      "nicht nicht PART\n",
      ", -- PUNCT\n",
      "weil weil SCONJ\n",
      "er er PRON\n",
      "Radios Radios NOUN\n",
      "und und CCONJ\n",
      "DVB-T-Empfänger DVB-T-Empfäng NUM\n",
      "stört stören VERB\n",
      ". -- PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['den Stadien', 'ich', 'Glück', 'noch keine wüsten Szenen', 'Vorstandschef Timotheus Höttges', 'sich', 'Analyst Robert Grindle', 'einer Studie', 'Montag', 'der dortigen Räterepublik', 'er', 'dem Krieg', 'Künstlergruppen', 'Ausschüssen', 'Welches Ergebnis', 'die Diskussion', 'EU-Ebene', 'die Preisentwicklung', 'dieMenschen', 'Weitere Informationen', 'www.schnippenburg.de', 'www.eisenzeithaus.de', 'neue Nachrichten', 'noz.de', 'die Startseite', \"Der Initiative 'Zivilcourage\", 'die', 'sich', 'das Denkmal', 'Form', 'Der islamistischen Szene', 'Thüringens', 'Angaben', 'des Thüringer Innenministeriums', 'etwa 125 Personen', 'der salafistischen Szene', 'etwa 75 Personen', 'er', 'die EMV-Prüfung', 'er', 'Radios']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем любой текст\n",
    "text = \"ДАННОЕ СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО \"\\\n",
    "       \"ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ \"\\\n",
    "       \"ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, \"\\\n",
    "       \"ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ДАННОЕ данное NOUN\n",
      "СООБЩЕНИЕ сообщение PROPN\n",
      "( ( PUNCT\n",
      "МАТЕРИАЛ материал PROPN\n",
      ") ) PUNCT\n",
      "СОЗДАНО создано PROPN\n",
      "И и PROPN\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РАСПРОСТРАНЕНО распространено PROPN\n",
      "ИНОСТРАННЫМ иностранным PROPN\n",
      "СРЕДСТВОМ средством PROPN\n",
      "МАССОВОЙ массовой PROPN\n",
      "ИНФОРМАЦИИ информации PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агента PROPN\n",
      ", , PUNCT\n",
      "И и CCONJ\n",
      "( ( PUNCT\n",
      "ИЛИ или PROPN\n",
      ") ) PUNCT\n",
      "РОССИЙСКИМ российским PROPN\n",
      "ЮРИДИЧЕСКИМ юридическим PROPN\n",
      "ЛИЦОМ лицом PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ выполняющим PROPN\n",
      "ФУНКЦИИ функция PROPN\n",
      "ИНОСТРАННОГО иностранного PROPN\n",
      "АГЕНТА агент PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
